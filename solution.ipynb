{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook. For example:\n",
    "\n",
    "```\n",
    "class SmoothOperator(Node):\n",
    "  def __init__(self):\n",
    "    super(SmoothOperator,self).__init__()\n",
    "    self.arity = 1\n",
    "    self.symb = \"SmoothOperator\"\n",
    "\n",
    "  def _get_args_repr(self, args):\n",
    "    return self._get_typical_repr(args,'before')\n",
    "\n",
    "  def get_output(self, X):\n",
    "    c_outs = self._get_child_outputs(X)\n",
    "    return np.smoothOperation(c_outs[0])\n",
    "\n",
    "  def get_output_pt(self, X):\n",
    "    c_outs = self._get_child_outputs_pt(X)\n",
    "    return torch.smoothOperation(c_outs[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised, used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Wind\n",
    "\n",
    "Wind can be added to the current environment setup as below:\n",
    "\n",
    "```bash\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False,\n",
    "               enable_wind=True, wind_power=15.0, turbulence_power=1.5)\n",
    "```\n",
    "\n",
    "Selin: When we add wind as a variable, it makes sense to also edit our fitness function. We can define a new boolean parameter (say have_random_wind) and if it is set to true when the fitness function is called, we can re-define the environment with a random wind value at each episode. This would potentially make our GP algorithm more robust to randomness. A possible implementation:\n",
    "\n",
    "```bash\n",
    "if use_random_wind:\n",
    "    env = gym.make(\"LunarLander-v3\", continuous=False,\n",
    "                    enable_wind=True,\n",
    "                    wind_power=np.random.uniform(5.0, 20.0),\n",
    "                    turbulence_power=np.random.uniform(0.5, 2.0))\n",
    "```\n",
    "\n",
    "The above can be added within the loop that goes over the episodes (the outer for loop of the fitness function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new Atomic Functions\n",
    "\n",
    "These atomic functions will be added as internal nodes within the Evolution setup.\n",
    "\n",
    "We can add min & max operators. Or instead, we can add **Clamp(x, min, max)** operator. This could be interesting.\n",
    "\n",
    "We can add domain specific operators:\n",
    "- Maybe a function that calculates the angle of the lunarlander to the pad (angle_to_pad(x_pos, y_pos)?)\n",
    "\n",
    "#### Fitness Calculation\n",
    "\n",
    "For the final fitness calculation, we are taking the sum of the rewards across episodes. Instead of sum operation, can we do this fitness calculation in a more clever way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other):\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selin's Notes\n",
    "\n",
    "**Fitness Function:** Fitness is defined as the cumulative reward of the landing.\n",
    "\n",
    "**Multitree:** Multitree contains 4 trees, one for each action of the Lunar lander. \n",
    "\n",
    "- `0`: do nothing\n",
    "- `1`: fire left orientation engine\n",
    "- `2`: fire main engine\n",
    "- `3`: fire right orientation engine\n",
    "\n",
    "Multitree is initialized under genepro/variation.py file, with the generate_random_multitree() method. This method is called in the genepro/evo.py file within the _initialize_population() internal method. \n",
    "\n",
    "**Understanding the Input Sample:** Input sample is an 8-dimensional vector: [x, y, vx, vy, angle, angular_velocity, leg1_contact, leg2_contact]\n",
    "\n",
    "e.g. [-2.5, -2.5, -10, -10, -6.28, -10, 0, 0]\n",
    "\n",
    "- index0 : x position of the lander\n",
    "- index1: y position of the lander\n",
    "- index2: velocity in the x direction\n",
    "- index3: velocity in the y direction\n",
    "- index4: angle of the lander\n",
    "- index5: angular velocity\n",
    "- index6: leg 1 in contact\n",
    "- index7: leg 2 in contact\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fitness_function_pt(reduction='sum', has_wind=False):\n",
    "  '''\n",
    "  Generic fitness function factory\n",
    "  '''\n",
    "  def fitness_function(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "    memory = ReplayMemory(10000)\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "      # define a new environment\n",
    "      if has_wind: \n",
    "        env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\",\n",
    "                        enable_wind=True,\n",
    "                        wind_power=np.random.uniform(0.0, 2.0), # TODO: Update the parameters for wind if necessary\n",
    "                        turbulence_power=np.random.uniform(0.0, 1.0)) # TODO: Update the parameters for turbulence if necessary\n",
    "      else:\n",
    "        env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "        \n",
    "      observation = env.reset()\n",
    "      observation = observation[0]\n",
    "      \n",
    "      for _ in range(episode_duration):\n",
    "        if render:\n",
    "          frames.append(env.render())\n",
    "\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float() # Input sample is a torch tensor\n",
    "\n",
    "        # what goes here? TODO\n",
    "        '''Below is Selin's possible definition of an action'''\n",
    "        output_scores = multitree.get_output_pt(input_sample) # A tensor of length 4, storing the scores of each action (after evaluating each tree)\n",
    "        action = torch.argmax(output_scores, dim=1) # Select the action with the highest score\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "        if (terminated or truncated) and not ignore_done:\n",
    "          break\n",
    "          \n",
    "    # Define the reward types here\n",
    "    # TODO: Add more reward types!\n",
    "    if reduction == 'sum':\n",
    "      fitness = np.sum(rewards)\n",
    "    elif reduction == 'min':\n",
    "      fitness = np.min(rewards)\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown reduction method: {reduction}\")\n",
    "    \n",
    "    return fitness, memory\n",
    "  \n",
    "  return fitness_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selin's ideas about the things they wanted us to consider in the evolution setup (possible areas for improvement)\n",
    "\n",
    "In the below code, it says **Think about the probability of sampling a coefficient (which is basically a constant).** Currently, in the below code, we have 8 features and we are adding all of them as leaf nodes. \n",
    "\n",
    "However, we are only adding 1 constant as a leaf node. So this would give us a 1/9 chance of sampling a coefficient. This is a very small probability. Hence, **a possible area of improvement** might be to consider adding more constants (e.g. 4 or 5 constants) to our leaf nodes set and see how our GP performs. I think having constant is **important** because they allow the model to shift or scale features (e.g. x_4 + 1.5).\n",
    "\n",
    "**Having more operators:** Currently, we only have basic arithmetic operators. We can add the following non-linear operators:\n",
    "- log\n",
    "- sqrt\n",
    "- sin, cos\n",
    "- max, min\n",
    "- exp\n",
    "- square, cube, ...\n",
    "\n",
    "**Adjusting the parameters of the Evolution() method** called below. We can design an experiment to find the best combination of parameter values for population size, max generations, and max tree size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)]\n",
    "leaf_nodes = leaf_nodes + [Constant()] # Think about the probability of sampling a coefficient\n",
    "internal_nodes = [Plus(),Minus(),Times(),Div()] # Add your own operators here\n",
    "\n",
    "fitness_function_pt = make_fitness_function_pt(reduction='sum') # Baseline fitness function\n",
    "\n",
    "evo = Evolution(\n",
    "  fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "  4,\n",
    "  pop_size=16,\n",
    "  max_gens=10,\n",
    "  max_tree_size=31,\n",
    "  n_jobs=8,\n",
    "  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolve\n",
    "Running this cell will use all the settings above as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSION:  [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gen: 1,\tbest of gen fitness: -591.651,\tbest of gen size: 25\n",
      "\n",
      "DIVERSION:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "gen: 2,\tbest of gen fitness: -967.040,\tbest of gen size: 23\n"
     ]
    }
   ],
   "source": [
    "best_fitnesses_across_gens = evo.evolve(is_multiobjective=True)\n",
    "\n",
    "# TODO: Should we account for the range difference between the two objectives? (diversity is in the range of 10s to 20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-591.6510316036314, -967.0402682307597]\n"
     ]
    }
   ],
   "source": [
    "# Print best fitnesses across generations\n",
    "print(best_fitnesses_across_gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['((((x_0-x_7)+x_5)/((x_2-x_2)/(x_1+x_1)))/((x_3*3.5308659076690674)/(x_0-(x_1*x_6))))', '(((((x_7/-3.919255018234253)/x_1)-(x_1*x_0))/(x_3+((x_5*x_7)/-3.939394950866699)))+(-2.5180726051330566+(x_5-x_4)))', '((((x_6-x_5)*(x_5*x_5))-x_6)+(((x_5*x_7)*(x_7-x_4))+(x_6*(-2.6383843421936035+x_2))))', '((x_4+((x_7*-4.614616394042969)*x_5))+((x_7*(x_6+x_1))+((-3.729720115661621*-0.2017134577035904)+(x_0-x_0))))']\n",
      "-3611.3446437338903\n"
     ]
    }
   ],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        \n",
    "        # TODO: Again, define the action\n",
    "        '''Selin's idea of an action is added below'''\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(output, dim=1) # Select the action with the highest score\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Experimentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nConfiguring the fitness function\\nTEST 1.1 Single objective: Experiment with different quality metrics for fitness\\nTEST 1.2 Multi-objective: Include diversity as a second metric\\n\\nRequired additional arguments for fitness function: has_wind=False, reward_type='sum', is_multiobjective=False\\nwhere reward_types = ['sum', 'min', 'weighted_sum']\\n\\nWind Test \\nAdding wind as a variable, and also adding a random wind value at each episode\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write down ideas for experimentation here!\n",
    "\n",
    "# TEST TYPE 1. Improving the Fitness Function\n",
    "'''\n",
    "Configuring the fitness function\n",
    "TEST 1.1 Single objective: Experiment with different quality metrics for fitness\n",
    "TEST 1.2 Multi-objective: Include diversity as a second metric\n",
    "\n",
    "Required additional arguments for fitness function: has_wind=False, reward_type='sum', is_multiobjective=False\n",
    "where reward_types = ['sum', 'min', 'weighted_sum']\n",
    "\n",
    "Wind Test \n",
    "Adding wind as a variable, and also adding a random wind value at each episode\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment for reward type: sum\n",
      "\n",
      "DIVERSION:  [25, 6, 6, 25, 0, 19, 10, 25, 19, 8, 10, 19, 8, 25, 0, 25]\n",
      "gen: 1,\tbest of gen fitness: -447.104,\tbest of gen size: 31\n",
      "\n",
      "DIVERSION:  [5, 24, 29, 4, 29, 29, 5, 35, 29, 29, 29, 25, 35, 31, 25, 29]\n",
      "gen: 2,\tbest of gen fitness: -547.849,\tbest of gen size: 29\n",
      "\n",
      "Running experiment for reward type: min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 93\u001b[0m\n\u001b[1;32m     88\u001b[0m         test_fitnesses\u001b[38;5;241m.\u001b[39mappend(best_test)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_fitnesses, test_fitnesses, reward_types\n\u001b[0;32m---> 93\u001b[0m training_fitnesses, test_fitnesses, reward_types \u001b[38;5;241m=\u001b[39m \u001b[43mfitness_function_reward_types_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_gens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 81\u001b[0m, in \u001b[0;36mfitness_function_reward_types_exp\u001b[0;34m(max_gens)\u001b[0m\n\u001b[1;32m     79\u001b[0m fitness_function_version \u001b[38;5;241m=\u001b[39m make_fitness_function_pt(reduction\u001b[38;5;241m=\u001b[39mreward_type)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning experiment for reward type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m best_training, best_test \u001b[38;5;241m=\u001b[39m \u001b[43mmain_experimentation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitness_function_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43minternal_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mleaf_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mmax_gens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mmax_tree_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m training_fitnesses\u001b[38;5;241m.\u001b[39mappend(best_training)\n\u001b[1;32m     88\u001b[0m test_fitnesses\u001b[38;5;241m.\u001b[39mappend(best_test)\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mmain_experimentation_loop\u001b[0;34m(fitness_function_version, internal_nodes, leaf_nodes, pop_size, max_gens, max_tree_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m evo \u001b[38;5;241m=\u001b[39m Evolution(\n\u001b[1;32m      8\u001b[0m         fitness_function_version, \n\u001b[1;32m      9\u001b[0m         internal_nodes, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;66;03m# Number of jobs is fixed to 8\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run the training loop for this evolution setup and get the fitnesses of the best individuals across generations\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m best_fitnesses_training \u001b[38;5;241m=\u001b[39m \u001b[43mevo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Now, run the testing loop for this evolution setup and get the fitnesses of the best individuals across generations\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract best-of-generation trees\u001b[39;00m\n\u001b[1;32m     23\u001b[0m best_individuals \u001b[38;5;241m=\u001b[39m evo\u001b[38;5;241m.\u001b[39mbest_of_gens\n",
      "File \u001b[0;32m~/Desktop/lunarlanding/lunarlanding/genepro/evo.py:287\u001b[0m, in \u001b[0;36mEvolution.evolve\u001b[0;34m(self, is_multiobjective)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# generational loop\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_terminate():\n\u001b[1;32m    286\u001b[0m   \u001b[38;5;66;03m# perform one generation\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_multiobjective\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m   \u001b[38;5;66;03m# log info\u001b[39;00m\n\u001b[1;32m    289\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Desktop/lunarlanding/lunarlanding/genepro/evo.py:208\u001b[0m, in \u001b[0;36mEvolution._perform_generation\u001b[0;34m(self, is_multiobjective)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# evaluate diversity for current population\u001b[39;00m\n\u001b[1;32m    207\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m [ind\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation]\n\u001b[0;32m--> 208\u001b[0m diversities_reverted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_diversities\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m diversities \u001b[38;5;241m=\u001b[39m [diversity \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m diversity \u001b[38;5;129;01min\u001b[39;00m diversities_reverted]\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# select promising parents when single objective (aka fitness)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/lunarlanding/lunarlanding/genepro/evo.py:313\u001b[0m, in \u001b[0;36mEvolution.calculate_diversities\u001b[0;34m(self, offspring_population)\u001b[0m\n\u001b[1;32m    310\u001b[0m pairs \u001b[38;5;241m=\u001b[39m [(i, j) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, n)]\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Parallel computation of similarities\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairwise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Aggregate results\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j, sim \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Think about how to incorporate wind to the experimentation setup!!!\n",
    "\n",
    "def main_experimentation_loop(fitness_function_version, internal_nodes, leaf_nodes,\n",
    "                                pop_size=16, max_gens=10, max_tree_size=31):\n",
    "            \n",
    "    # Initialize an evolution setup with the correct fitness function\n",
    "    evo = Evolution(\n",
    "            fitness_function_version, \n",
    "            internal_nodes, \n",
    "            leaf_nodes,\n",
    "            n_trees=4, # Number of trees in multitree is fixed to 4\n",
    "            pop_size=pop_size,\n",
    "            max_gens=max_gens,\n",
    "            max_tree_size=max_tree_size,\n",
    "            n_jobs=8, # Number of jobs is fixed to 8\n",
    "            verbose=True)\n",
    "    \n",
    "    # Run the training loop for this evolution setup and get the fitnesses of the best individuals across generations\n",
    "    best_fitnesses_training = evo.evolve()\n",
    "\n",
    "    # Now, run the testing loop for this evolution setup and get the fitnesses of the best individuals across generations\n",
    "    # Extract best-of-generation trees\n",
    "    best_individuals = evo.best_of_gens\n",
    "\n",
    "    # Collect test scores for each best individual\n",
    "    best_fitnesses_test = [\n",
    "        get_test_score(ind) for ind in best_individuals\n",
    "    ]\n",
    "\n",
    "    return best_fitnesses_training, best_fitnesses_test\n",
    "\n",
    "\n",
    "def plot_fitnesses(training_fitnesses, test_fitnesses, reward_types):\n",
    "    generations = list(range(len(training_fitnesses[0])))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Training fitness plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, reward_type in enumerate(reward_types):\n",
    "        plt.plot(generations, training_fitnesses[i], label=reward_type)\n",
    "    plt.title(\"Training Fitness over Generations\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Testing fitness plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, reward_type in enumerate(reward_types):\n",
    "        plt.plot(generations, test_fitnesses[i], label=reward_type)\n",
    "    plt.title(\"Testing Fitness over Generations\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# TEST TYPE 1. Improving the Fitness Function\n",
    "'''\n",
    "Configuring the fitness function\n",
    "TEST 1.1 Single objective: Experiment with different quality metrics for fitness\n",
    "TEST 1.2 Multi-objective: Include diversity as a second metric\n",
    "\n",
    "Required additional arguments for fitness function: has_wind=False, reward_type='sum', is_multiobjective=False\n",
    "where reward_types = ['sum', 'min', 'weighted_sum']\n",
    "\n",
    "Wind Test \n",
    "Adding wind as a variable, and also adding a random wind value at each episode\n",
    "''' \n",
    "def fitness_function_reward_types_exp(max_gens=10):\n",
    "    reward_types = ['sum', 'min']\n",
    "    training_fitnesses = []\n",
    "    test_fitnesses = []\n",
    "    for reward_type in reward_types:\n",
    "        fitness_function_version = make_fitness_function_pt(reduction=reward_type)\n",
    "        print(f\"\\nRunning experiment for reward type: {reward_type}\")\n",
    "        best_training, best_test = main_experimentation_loop(fitness_function_version, \n",
    "                                                             internal_nodes, \n",
    "                                                             leaf_nodes,\n",
    "                                                             pop_size=16, \n",
    "                                                             max_gens=max_gens, \n",
    "                                                             max_tree_size=31)\n",
    "        training_fitnesses.append(best_training)\n",
    "        test_fitnesses.append(best_test)\n",
    "\n",
    "    return training_fitnesses, test_fitnesses, reward_types\n",
    "\n",
    "\n",
    "training_fitnesses, test_fitnesses, reward_types = fitness_function_reward_types_exp(max_gens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
